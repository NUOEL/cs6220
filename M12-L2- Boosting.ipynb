{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a machine learning meta-algorithm for reducing bias in supervised learning. The procedure is intended to \"boost\" the performance of a weak learner (a classifier that is only slightly correlated with the true classification) into a strong learner (a classifier that is arbitrarily well-correlated with the true classification).\n",
    "\n",
    "While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learners' accuracy. After a weak learner is added, the data is reweighted:  examples that are misclassified gain weight (and examples that are classified correctly may lose weight). Thus, future weak learners focus more on the examples that previous weak learners misclassified.\n",
    "\n",
    "The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and perhaps the most significant historically as it was the first algorithm that could adapt to the weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# sklearn (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/weight_boosting.py)\n",
    "#\n",
    "# Generates a \"boosted\" ensemble of base models.\n",
    "\n",
    "import copy\n",
    "import operator\n",
    "import sys, math, random\n",
    "import numpy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoostClassifier(ClassifierMixin):\n",
    "    \"\"\"An AdaBoost classifier.\n",
    "\n",
    "    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
    "    classifier on the original dataset and then fits additional copies of the\n",
    "    classifier on the same dataset but where the weights of incorrectly\n",
    "    classified instances are adjusted such that subsequent classifiers focus\n",
    "    more on difficult cases.\n",
    "\n",
    "    This class implements the algorithm known as AdaBoost-SAMME [2].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "\n",
    "    n_estimators : integer, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `estimators_` : list of classifiers\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    `classes_` : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "\n",
    "    `n_classes_` : int\n",
    "        The number of classes.\n",
    "\n",
    "    `estimator_weights_` : array of floats\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    `estimator_errors_` : array of floats\n",
    "        Classification error for each estimator in the boosted\n",
    "        ensemble.\n",
    "\n",
    "    `feature_importances_` : array of shape = [n_features]\n",
    "        The feature importances if supported by the ``base_estimator``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
    "           on-Line Learning and an Application to Boosting\", 1995.\n",
    "\n",
    "    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R'):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_params = estimator_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def _make_estimator(self, append=True):\n",
    "        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n",
    "\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = copy.deepcopy(self.base_estimator)\n",
    "        estimator.set_params(**dict((p, getattr(self, p))\n",
    "                                    for p in self.estimator_params))\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        # Check that the sample weights sum is positive.\n",
    "        if sample_weight.sum() <= 0:\n",
    "            raise ValueError(\n",
    "                \"Attempting to fit with a non-positive \"\n",
    "                \"weighted number of samples.\")\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            #print 'Iteration [%s]' % (iboost)\n",
    "\n",
    "            # Fit the estimator.\n",
    "            estimator = self._make_estimator()\n",
    "            estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "            if iboost == 0:\n",
    "                self.classes_ = getattr(estimator, 'classes_', None)\n",
    "                self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            # Generate estimator predictions.\n",
    "            y_pred = estimator.predict(X)\n",
    "\n",
    "            # Instances incorrectly classified.\n",
    "            incorrect = y_pred != y\n",
    "\n",
    "            # Error fraction.\n",
    "            estimator_error = np.mean(\n",
    "                np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "            # Boost weight using multi-class AdaBoost SAMME alg.\n",
    "            estimator_weight = self.learning_rate * (\n",
    "                np.log((1. - estimator_error) / estimator_error) +\n",
    "                np.log(self.n_classes_ - 1.))\n",
    "\n",
    "            # Only boost the weights if there is another iteration of fitting.\n",
    "            if not iboost == self.n_estimators - 1:\n",
    "                # Only boost positive weights (exponential loss).\n",
    "                sample_weight *= np.exp(estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) |\n",
    "                                         (estimator_weight < 0)))\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if not hasattr(self, \"estimators_\"):\n",
    "            raise ValueError(\"Call 'fit' first.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = numpy.array(X)\n",
    "        N, d = X.shape\n",
    "        pred = numpy.zeros(N)\n",
    "        for estimator, w in zip(self.estimators_, self.estimator_weights_):\n",
    "            pred += estimator.predict(X) * w\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            return pred.sum(axis=1)\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba <= 0] = 1e-5\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                           * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import operator\n",
    "import sys, math, random\n",
    "import numpy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class LogitBoostClassifier(ClassifierMixin):\n",
    "    \"\"\"A LogitBoost classifier.\n",
    "\n",
    "    A LogitBoost [1] classifier is a meta-estimator that begins by fitting a\n",
    "    classifier on the original dataset and then fits additional copies of the\n",
    "    classifier on the same dataset but where the weights of incorrectly\n",
    "    classified instances are adjusted such that subsequent classifiers focus\n",
    "    more on difficult cases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "\n",
    "    n_estimators : integer, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `estimators_` : list of classifiers\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    `classes_` : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "\n",
    "    `n_classes_` : int\n",
    "        The number of classes.\n",
    "\n",
    "    `estimator_weights_` : array of floats\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    `estimator_errors_` : array of floats\n",
    "        Classification error for each estimator in the boosted\n",
    "        ensemble.\n",
    "\n",
    "    `feature_importances_` : array of shape = [n_features]\n",
    "        The feature importances if supported by the ``base_estimator``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Friedman, T. Hastie, R. Tibshirani, \"Additive Logistic Regression: \n",
    "           A Statistical View of Boosting\", 2000.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R'):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_params = estimator_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def _make_estimator(self, append=True):\n",
    "        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n",
    "\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = copy.deepcopy(self.base_estimator)\n",
    "        estimator.set_params(**dict((p, getattr(self, p))\n",
    "                                    for p in self.estimator_params))\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        # Check that the sample weights sum is positive.\n",
    "        if sample_weight.sum() <= 0:\n",
    "            raise ValueError(\n",
    "                \"Attempting to fit with a non-positive \"\n",
    "                \"weighted number of samples.\")\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            #print 'Iteration [%s]' % (iboost)\n",
    "\n",
    "            # Fit the estimator.\n",
    "            estimator = self._make_estimator()\n",
    "            estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "            if iboost == 0:\n",
    "                self.classes_ = getattr(estimator, 'classes_', None)\n",
    "                self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            # Generate estimator predictions.\n",
    "            y_pred = estimator.predict(X)\n",
    "\n",
    "            # Instances incorrectly classified.\n",
    "            incorrect = y_pred != y\n",
    "\n",
    "            # Error fraction.\n",
    "            estimator_error = np.mean(\n",
    "                np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "            # Boost weight using multi-class AdaBoost SAMME alg.\n",
    "            estimator_weight = self.learning_rate * (\n",
    "                np.log((1. - estimator_error) / estimator_error) +\n",
    "                np.log(self.n_classes_ - 1.))\n",
    "\n",
    "            # Only boost the weights if there is another iteration of fitting.\n",
    "            if not iboost == self.n_estimators - 1:\n",
    "                # Only boost positive weights (logistic loss).\n",
    "                sample_weight *= np.log(1 + np.exp(estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) |\n",
    "                                         (estimator_weight < 0))))\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if not hasattr(self, \"estimators_\"):\n",
    "            raise ValueError(\"Call 'fit' first.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = numpy.array(X)\n",
    "        N, d = X.shape\n",
    "        pred = numpy.zeros(N)\n",
    "        for estimator, w in zip(self.estimators_, self.estimator_weights_):\n",
    "            pred += estimator.predict(X) * w\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            return pred.sum(axis=1)\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.log(1 + np.exp((1. / (n_classes - 1)) * proba))\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba <= 0] = 1e-5\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                           * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will demonstrate boosting by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding method available via scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encode = True\n",
    "\n",
    "# Load the Iris flower dataset\n",
    "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris = pd.read_csv(fileURL, \n",
    "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
    "                   header=None)\n",
    "iris = iris.dropna()\n",
    "\n",
    "X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] # features\n",
    "labels = iris['Species'] # class\n",
    "\n",
    "if label_encode:\n",
    "    # Transform string (nominal) output to numeric\n",
    "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "else:\n",
    "    y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we partition the dataset into non-overlapping training and testing sets, with 40% of the data allocated to the training set and 60% allocated to the testing set. All of the training for the boosting algorithms will be performed on the training set, while the testing set will be used solely to generate predictions, the accuracy of which we will later evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The training sets will be used for all training and validation purposes.\n",
    "# The testing sets will only be used for evaluating the final blended (level 1) classifier.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our boosting algorithms will iteratatively generate an ensemble of base (weak) learners. At each iteration, the a base learner will generate predictions on the set of instances. The learners accuracy on these instances will be used to weight its predictions in the ensemble and to reweight the instances for the next iteration. AdaBoost and LogitBoost differ in the how they adjust the instance weights at each iteration.\n",
    "\n",
    "Once the base models are trained, they are combined into an ensemble. This means that their outputs (predictions) are weighted an combined to predict the target or class variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=100)\n",
    "abc.fit(X_train, y_train)\n",
    "\n",
    "lbc = LogitBoostClassifier(n_estimators=100)\n",
    "lbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the performance of the boosted models to the base (weak) ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy = 0.955555555556\n",
      "LogitBoost Accuracy = 0.955555555556\n",
      "Base Classifier Accuracy = 0.977777777778\n",
      "sklearn AdaBoost Accuracy = 0.944444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "### Generate predictions with AdaBoost classifier. ###\n",
    "\n",
    "score = metrics.accuracy_score(y_test, abc.predict(X_test))\n",
    "print ('AdaBoost Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "### Generate predictions with LogitBoost classifier. ###\n",
    "\n",
    "score = metrics.accuracy_score(y_test, lbc.predict(X_test))\n",
    "print ('LogitBoost Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "### Generate predictions with base (non-boosted) classifier. ###\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "score = metrics.accuracy_score(y_test, clf.predict(X_test))\n",
    "print ('Base Classifier Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "import sklearn.ensemble\n",
    "clf2 = sklearn.ensemble.AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "score = metrics.accuracy_score(y_test, clf2.predict(X_test))\n",
    "print ('sklearn AdaBoost Accuracy = %s' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by using the method of boosting, we generate a \"meta\" model that can outperform each of the base models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
